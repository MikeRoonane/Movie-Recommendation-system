✅ Phase 1: The Foundation - Database and Manual Model Training
🔧 Infrastructure Setup
 ✔ Create project folder: movie-recommendation-platform/ @done(25-07-29 10:54)

 ✔ Initialize Git repository @done(25-07-29 10:54)
 
 ✔ Add .devcontainer/ with Python + Poetry features @done(25-07-29 10:54)
 
 ☐ Add .gitignore and .env
 
 ✔ Create minimal docker-compose.yml for PostgreSQL @done(25-07-29 13:31)
 
 ☐ Add persistent volume for PostgreSQL
 
 ☐ Run: docker-compose up -d and verify container is running
 
 ☐ Test connection using psql or Python

🛠️ SQLAlchemy Basics

 ☐ Learn SQLAlchemy ORM basics (official docs or tutorial)

 ☐ Create core/models.py or core/db/models.py
 
 ☐ Define Movie and Rating ORM classes
 
 ☐ Create get_engine() and get_session() helper functions
 
 ☐ Initialize tables with Base.metadata.create_all(engine)

📥 Data Seeding

 ☐ Download MovieLens dataset

 ☐ Unzip and move movies.csv and ratings.csv to scripts/seed_data/
 
 ☐ Create scripts/seed_database.py
 
 ☐ Use pandas to load CSVs
 
 ☐ Use SQLAlchemy to insert into DB
 
 ☐ Run script: python scripts/seed_database.py
 
 ☐ Inspect database (optional: use pgAdmin or DBeaver)

 🎓 Model Training

 ☐ Create services/training_service/app/main.py

 ☐ Connect to database and read ratings into a pandas.DataFrame
 
 ☐ Train basic model using surprise.SVD or similar
 
 ☐ Evaluate model (RMSE)
 
 ☐ Save model to local file using joblib.dump(model, "model.pkl")

✅ Phase 1 Complete When:

 Data is in PostgreSQL

 ORM works

 Model trains manually and saves to disk

✅ Phase 2: MLOps – MLflow and Airflow Integration
🔧 Add Services
 Update docker-compose.yml to include:

 MLflow server

 Airflow with PostgreSQL backend

 Add volumes for Airflow and MLflow

 Confirm MLflow is running at http://localhost:5000

 Confirm Airflow is running at http://localhost:8080

📦 MLflow Integration
 Install mlflow, psycopg2-binary, boto3

 Update training script to:

 Use mlflow.start_run()

 Log hyperparams with mlflow.log_param()

 Log metrics with mlflow.log_metric()

 Save model with mlflow.sklearn.log_model() or custom method

 Verify run appears in MLflow UI

🌀 Airflow Integration
 Create dags/training_dag.py

 Create BashOperator to run training script

 Load DAG in Airflow UI

 Trigger DAG manually and verify success

 Verify run logs in MLflow

✅ Phase 2 Complete When:

 You can click a button in Airflow to train a model

 MLflow logs every run

✅ Phase 3: Real-Time – Kafka and Redis Integration
🛰️ Kafka + Redis Setup
 Add Kafka and Redis to docker-compose.yml

 Set up Kafka topics: new-movies, user-feedback

 Use confluent-kafka or kafka-python to interact

 Use redis-py for Redis access

 Test Kafka connection using a test producer/consumer

🔮 Inference Service
 Create services/inference_service/app/main.py

 Subscribe to new-movies topic

 On message:

 Load latest model from MLflow

 Generate recommendations

 Store in Redis with key recs:movie_id_{id}

🧪 Test Producer Script
 Create scripts/send_test_movie.py

 Send a sample movie ID to Kafka

 Confirm Redis receives recommendations

✅ Phase 3 Complete When:

 Kafka + Redis running

 Sending a new movie triggers Redis cache update

✅ Phase 4: User-Facing API – FastAPI Integration
🌐 API Service Setup
 Create services/api/app/main.py

 Add routers/ for endpoints

 Add Pydantic models for request/response

📤 GET /recommendations/{user_id}
 Read Redis key recs:user_id_{id}

 Return recommendations as JSON

📥 POST /feedback
 Accept user feedback (liked, rated, skipped)

 Package into Kafka message

 Send to user-feedback Kafka topic

🧪 Test API
 Use curl or Postman to test endpoints

 Check Redis for GET

 Check Kafka for POST

✅ Phase 4 Complete When:

 API returns recommendations

 Feedback can be posted and is sent to Kafka

🧾 Optional Extras for Professional Polish
 Set up pre-commit hooks (black, ruff, mypy)

 Add Alembic for DB schema migrations

 Add health check endpoints in API

 Set up a Makefile or shell script to simplify local dev

 Add CI/CD with .github/workflows/lint-and-test.yml

📦 Bonus: Final Delivery Checklist
 Code is modular, clean, and documented

 .env, .venv/, __pycache__/ are gitignored

 README.md explains architecture, setup, usage

 docker-compose.yml works end-to-end

 pyproject.toml has all dependencies

 Logs go to console via core/logger.py