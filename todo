âœ… Phase 1: The Foundation - Database and Manual Model Training
ğŸ”§ Infrastructure Setup
 âœ” Create project folder: movie-recommendation-platform/ @done(25-07-29 10:54)

 âœ” Initialize Git repository @done(25-07-29 10:54)
 
 âœ” Add .devcontainer/ with Python + Poetry features @done(25-07-29 10:54)
 
 â˜ Add .gitignore and .env
 
 âœ” Create minimal docker-compose.yml for PostgreSQL @done(25-07-29 13:31)
 
 â˜ Add persistent volume for PostgreSQL
 
 â˜ Run: docker-compose up -d and verify container is running
 
 â˜ Test connection using psql or Python

ğŸ› ï¸ SQLAlchemy Basics

 â˜ Learn SQLAlchemy ORM basics (official docs or tutorial)

 â˜ Create core/models.py or core/db/models.py
 
 â˜ Define Movie and Rating ORM classes
 
 â˜ Create get_engine() and get_session() helper functions
 
 â˜ Initialize tables with Base.metadata.create_all(engine)

ğŸ“¥ Data Seeding

 â˜ Download MovieLens dataset

 â˜ Unzip and move movies.csv and ratings.csv to scripts/seed_data/
 
 â˜ Create scripts/seed_database.py
 
 â˜ Use pandas to load CSVs
 
 â˜ Use SQLAlchemy to insert into DB
 
 â˜ Run script: python scripts/seed_database.py
 
 â˜ Inspect database (optional: use pgAdmin or DBeaver)

 ğŸ“ Model Training

 â˜ Create services/training_service/app/main.py

 â˜ Connect to database and read ratings into a pandas.DataFrame
 
 â˜ Train basic model using surprise.SVD or similar
 
 â˜ Evaluate model (RMSE)
 
 â˜ Save model to local file using joblib.dump(model, "model.pkl")

âœ… Phase 1 Complete When:

 Data is in PostgreSQL

 ORM works

 Model trains manually and saves to disk

âœ… Phase 2: MLOps â€“ MLflow and Airflow Integration
ğŸ”§ Add Services
 Update docker-compose.yml to include:

 MLflow server

 Airflow with PostgreSQL backend

 Add volumes for Airflow and MLflow

 Confirm MLflow is running at http://localhost:5000

 Confirm Airflow is running at http://localhost:8080

ğŸ“¦ MLflow Integration
 Install mlflow, psycopg2-binary, boto3

 Update training script to:

 Use mlflow.start_run()

 Log hyperparams with mlflow.log_param()

 Log metrics with mlflow.log_metric()

 Save model with mlflow.sklearn.log_model() or custom method

 Verify run appears in MLflow UI

ğŸŒ€ Airflow Integration
 Create dags/training_dag.py

 Create BashOperator to run training script

 Load DAG in Airflow UI

 Trigger DAG manually and verify success

 Verify run logs in MLflow

âœ… Phase 2 Complete When:

 You can click a button in Airflow to train a model

 MLflow logs every run

âœ… Phase 3: Real-Time â€“ Kafka and Redis Integration
ğŸ›°ï¸ Kafka + Redis Setup
 Add Kafka and Redis to docker-compose.yml

 Set up Kafka topics: new-movies, user-feedback

 Use confluent-kafka or kafka-python to interact

 Use redis-py for Redis access

 Test Kafka connection using a test producer/consumer

ğŸ”® Inference Service
 Create services/inference_service/app/main.py

 Subscribe to new-movies topic

 On message:

 Load latest model from MLflow

 Generate recommendations

 Store in Redis with key recs:movie_id_{id}

ğŸ§ª Test Producer Script
 Create scripts/send_test_movie.py

 Send a sample movie ID to Kafka

 Confirm Redis receives recommendations

âœ… Phase 3 Complete When:

 Kafka + Redis running

 Sending a new movie triggers Redis cache update

âœ… Phase 4: User-Facing API â€“ FastAPI Integration
ğŸŒ API Service Setup
 Create services/api/app/main.py

 Add routers/ for endpoints

 Add Pydantic models for request/response

ğŸ“¤ GET /recommendations/{user_id}
 Read Redis key recs:user_id_{id}

 Return recommendations as JSON

ğŸ“¥ POST /feedback
 Accept user feedback (liked, rated, skipped)

 Package into Kafka message

 Send to user-feedback Kafka topic

ğŸ§ª Test API
 Use curl or Postman to test endpoints

 Check Redis for GET

 Check Kafka for POST

âœ… Phase 4 Complete When:

 API returns recommendations

 Feedback can be posted and is sent to Kafka

ğŸ§¾ Optional Extras for Professional Polish
 Set up pre-commit hooks (black, ruff, mypy)

 Add Alembic for DB schema migrations

 Add health check endpoints in API

 Set up a Makefile or shell script to simplify local dev

 Add CI/CD with .github/workflows/lint-and-test.yml

ğŸ“¦ Bonus: Final Delivery Checklist
 Code is modular, clean, and documented

 .env, .venv/, __pycache__/ are gitignored

 README.md explains architecture, setup, usage

 docker-compose.yml works end-to-end

 pyproject.toml has all dependencies

 Logs go to console via core/logger.py